{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIxD-3uDhmz8"
      },
      "source": [
        "# ***Text Generation by Recurrent Neural Network***\n",
        "---\n",
        "![neural-network-explained.jpeg](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5sk4ITHqNxL"
      },
      "source": [
        "# Data PreProcessing & Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd_8RKulksXK"
      },
      "source": [
        "## Import Essentials :\n",
        "\n",
        "- **Libraries Imported**:\n",
        "  - `numpy`: Used for numerical operations, specifically for handling arrays, which are crucial for data manipulation in machine learning.\n",
        "  - `tensorflow` & `keras`: Libraries essential for building and training deep learning models. Keras acts as a high-level API for TensorFlow.\n",
        "\n",
        "- **Sequential Model**:\n",
        "  - `Sequential`: A linear stack of layers, used for building neural networks where one layer feeds into the next sequentially.\n",
        "\n",
        "- **Key Layers Used**:\n",
        "  - `Dense`: Fully connected layer, used typically at the output layer of the model to make predictions.\n",
        "  - `Embedding`: Converts input data (e.g., words) into dense vectors of fixed size. Essential for word embeddings in text generation tasks.\n",
        "  - `SimpleRNN`: A Recurrent Neural Network layer. It processes sequential data (e.g., text) by maintaining an internal state, which helps the model \"remember\" previous inputs.\n",
        "  - `Flatten`: Flattens the input, converting multi-dimensional input into 1D, often used before passing data to fully connected layers.\n",
        "  - `InputLayer` & `Input`: Define the input shape of the model.\n",
        "\n",
        "- **Optimizers**:\n",
        "  - `RMSprop`: Optimizer used for recurrent neural networks, often effective in handling the vanishing gradient problem during training.\n",
        "  - `Adam`: Another optimizer often used for training deep learning models, combining the advantages of RMSprop and AdaGrad.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RrrYIc90VokG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import  Dense, Embedding, SimpleRNN, Flatten, InputLayer,Input, SimpleRNN\n",
        "from keras.optimizers import RMSprop, Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9hiJl35ne-_"
      },
      "source": [
        "## Loading & Manipulating Input Text\n",
        "\n",
        "- **File Uploading**:\n",
        "  - `tf.keras.utils.get_file`: Downloads a file from a given URL and stores it in the system's cache. In this case, the file is `shakespeare.txt`, which contains the text data (Shakespeare's works) used for text generation.\n",
        "\n",
        "- **Reading and Decoding Text**:\n",
        "  - `open(path_to_file, 'rb').read()`: Reads the file in binary mode (`'rb'`) to avoid encoding issues, and then the content is stored in `txt`.\n",
        "  - `txt.decode('utf-8')`: Decodes the binary data to readable UTF-8 encoded text, making it suitable for processing and analysis.\n",
        "\n",
        "- **Text Analysis**:\n",
        "  - **Length of the Text**:\n",
        "    - `len(txt)`: Calculates the total number of characters in the extracted text, providing the corpus size for training the RNN.\n",
        "    - `print(f\"Length of the text: {len(txt)} characters\")`: Displays the total number of characters.\n",
        "  \n",
        "  - **Displaying First 300 Characters**:\n",
        "    - `txt[:300]`: Slices and prints the first 300 characters from the text. This helps in understanding the content and structure of the dataset.\n",
        "  \n",
        "  - **Unique Characters**:\n",
        "    - `vocab = sorted(list(set(txt)))`:\n",
        "      - **Set**: Converts the text into a set to filter out repeated characters, since sets do not allow duplicates.\n",
        "      - **Sorted List**: Sorts the set into a list to make the unique characters usable for model training.\n",
        "      - The unique characters in the text serve as the vocabulary for the text generation model.\n",
        "  \n",
        "  - **Vocabulary Information**:\n",
        "    - `len(vocab)`: Calculates and prints the number of unique characters in the dataset.\n",
        "    - `print(vocab)`: Displays the unique characters, which include letters, punctuation, and other symbols.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7wcHSDUZVAc",
        "outputId": "873dad3c-e0c6-461c-a49d-efc5a8b3698d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Uploading a file\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hwao75QjZqI2"
      },
      "outputs": [],
      "source": [
        "#Extracting text from uploadedfile\n",
        "txt=open(path_to_file,'rb').read()\n",
        "#Decoding the extracted text\n",
        "txt=txt.decode(encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwWCPg2JaDnh",
        "outputId": "9cee820b-2d00-42f4-a74b-5fa8192197b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the text: 1115394 characters\n",
            "--------------------------------------------------\n",
            "First 300 character are as follows:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ],
      "source": [
        "#Analyzing the extracted decoded txt\n",
        "\n",
        "#No. of chars in txt\n",
        "print(f\"Length of the text: {len(txt)} characters\\n{'-'*50}\")\n",
        "\n",
        "# Displaying first  300 characters\n",
        "print(f\"First 300 character are as follows:\\n{txt[:300]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdykBEBTjbf0",
        "outputId": "bfba6906-8870-4a11-9bcd-972df93d736b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of unique characters are: 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "#unique characters\n",
        "vocab=sorted(list(set(txt))) #Converting txt to set 'cause set doesn't allow repetition thus we can easily get all the unique chars\n",
        "print(f\"The total number of unique characters are: {len(vocab)}\")\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh8E3bMZ6SII"
      },
      "source": [
        "##**Mapping the input data**\n",
        "\n",
        "- Index to Char\n",
        "- Char to Index\n",
        "- Index to Txt\n",
        "- Txt to Index\n",
        "\n",
        "## Explanation:\n",
        "\n",
        "- **Mapping Characters to Indices**:\n",
        "  - `index2char = {index: char for index, char in enumerate(vocab)}`:\n",
        "    - Creates a dictionary where each unique character in the vocabulary (`vocab`) is assigned an index.\n",
        "    - This dictionary is used to map indices back to characters, which is important during the text generation process when predicted indices need to be converted back to characters.\n",
        "\n",
        "  - `char2index = {char: index for index, char in enumerate(vocab)}`:\n",
        "    - Another dictionary that reverses the mapping, associating each character with its corresponding index.\n",
        "    - This is crucial for converting the text data into a format that can be fed into the model, i.e., numerical indices instead of raw characters.\n",
        "\n",
        "- **Converting Text to Indices**:\n",
        "  - `txt_to_index = np.array([char2index[char] for char in txt])`:\n",
        "    - Transforms the entire text into a sequence of numerical indices using the `char2index` dictionary.\n",
        "    - Each character in the text is replaced by its corresponding index, allowing the text to be processed by the RNN, which operates on numerical data.\n",
        "    - `np.array`: The resulting list of indices is converted into a NumPy array for efficient manipulation and model input.\n",
        "  \n",
        "  - `print(txt_to_index.shape)`:\n",
        "    - Displays the shape of the resulting array, showing the total number of characters in the text as indices.\n",
        "\n",
        "- **Converting Indices to Text**:\n",
        "  - `def index2txt(index):`:\n",
        "    - Defines a function that converts a sequence of indices back into text.\n",
        "    - `''.join(index2char[index])`: Joins the characters corresponding to the indices back into a string.\n",
        "    - `repr`: Displays the string in a representation format, which includes escape sequences (like `\\n` for new lines).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhOfuxwHkCbs",
        "outputId": "36440e6a-1354-4a67-9e52-176d53c3b3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1115394,)\n"
          ]
        }
      ],
      "source": [
        "#Index to char\n",
        "index2char={index:char for index,char in enumerate(vocab)}\n",
        "#Char to index\n",
        "char2index={char:index for index,char in enumerate(vocab)}\n",
        "\n",
        "#Txt to index\n",
        "txt_to_index=np.array([char2index[char] for char in txt])\n",
        "print(txt_to_index.shape)\n",
        "\n",
        "#Indices to txt\n",
        "def index2txt(index):\n",
        "  return repr(''.join(index2char[index]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na-tJ-I6GhCG"
      },
      "source": [
        "## Create training examples and targets\n",
        "\n",
        "\n",
        "\n",
        "- **Generating Sequences**:\n",
        "  - **Parameters**:\n",
        "    - `maxlen = 40`: Defines the length of each sequence of text that will be used as input for training. Each sequence contains 40 characters.\n",
        "    - `step = 3`: The step size for moving the window across the text. The sequence window moves 3 characters at a time to generate overlapping sequences.\n",
        "\n",
        "  - **Creating Sequences and Labels**:\n",
        "    - **Lists Initialization**:\n",
        "      - `sentences = []`: A list to store sequences of text.\n",
        "      - `next_chars = []`: A list to store the character immediately following each sequence.\n",
        "\n",
        "    - **Loop for Sequence Generation**:\n",
        "      - `for i in range(0, len(txt) - maxlen, step)`: Iterates over the text with a window of `maxlen` characters and a step size of `step`.\n",
        "        - `sentences.append(txt[i: i + maxlen])`: Extracts a sequence of `maxlen` characters starting from position `i` and appends it to `sentences`.\n",
        "        - `next_chars.append(txt[i + maxlen])`: Appends the character immediately following the sequence to `next_chars`.\n",
        "\n",
        "  - **Output**:\n",
        "    - `print(f\"No. of sequences: {len(sentences)}\")`: Prints the number of generated sequences, indicating how many sequences the RNN will be trained on.\n",
        "    - `print(f\"Len of next_chars: {len(next_chars)}\")`: Prints the length of `next_chars`, which should be equal to the number of sequences, showing the number of target characters for training.\n",
        "    - `print(len(txt) - maxlen)`: Shows the total number of possible sequences in the text, providing insight into the data coverage.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vdO6A2V9zql4"
      },
      "outputs": [],
      "source": [
        "#Generating Sequences\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(txt) - maxlen, step):\n",
        "    sentences.append(txt[i: i + maxlen])\n",
        "    next_chars.append(txt[i + maxlen])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD5Vc110qKEs",
        "outputId": "5210d7be-fa39-4404-b116-5c86f22c56d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of sequences: 371785\n",
            "Len of nex_chars: 371785\n",
            "1115354\n"
          ]
        }
      ],
      "source": [
        "print(f\"No. of sequences: {len(sentences)}\")\n",
        "print(f\"Len of nex_chars: {len(next_chars)}\")\n",
        "print(len(txt) - maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLUs7Jfxo4CC"
      },
      "source": [
        "## **Vectorization**\n",
        "\n",
        "\n",
        "  - **Creating Numerical Format Data**:\n",
        "    - `x = np.zeros((len(sentences), maxlen, len(vocab)), dtype=np.bool_)`:\n",
        "      - Initializes a NumPy array `x` with shape `(number_of_sequences, sequence_length, vocabulary_size)`.\n",
        "      - **Number of Sequences**: `len(sentences)`\n",
        "      - **Sequence Length**: `maxlen`\n",
        "      - **Vocabulary Size**: `len(vocab)`\n",
        "      - Each element is a boolean (`dtype=np.bool_`), used to represent the presence of a character in the sequence.\n",
        "\n",
        "    - `y = np.zeros((len(sentences), len(vocab)), dtype=np.bool_)`:\n",
        "      - Initializes a NumPy array `y` with shape `(number_of_sequences, vocabulary_size)`.\n",
        "      - Each element is a boolean (`dtype=np.bool_`), representing the target character for each sequence.\n",
        "\n",
        "  - **Filling the Data**:\n",
        "    - **Loop through Sequences**:\n",
        "      - `for i, sentence in enumerate(sentences)`: Iterates over each sequence in `sentences`.\n",
        "        - `for t, char in enumerate(sentence)`: Iterates over each character in the current sequence.\n",
        "          - `x[i, t, char2index[char]] = 1`: Sets the position corresponding to the character index to 1 in the `x` array, marking the presence of that character.\n",
        "        - `y[i, char2index[next_chars[i]]] = 1`: Sets the position corresponding to the next character index to 1 in the `y` array, marking the target character for the sequence.\n",
        "\n",
        "  - **Output**:\n",
        "    - `print(f\"Shape of x is : {x.shape}\")`: Prints the shape of the `x` array, which confirms the dimensions of the input data.\n",
        "    - `print(f\"Shape of y is : {y.shape}\")`: Prints the shape of the `y` array, confirming the dimensions of the target data.\n",
        "    - `print(f\"{'-'*50}\\nx[0] is given as:\\n{x[0]}\")`: Displays the one-hot encoded representation of the first sequence.\n",
        "    - `print(f\"{'-'*50}\\ny[0] is given as:\\n{y[0]}\")`: Displays the one-hot encoded representation of the target character for the first sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q5z2_pjBConu"
      },
      "outputs": [],
      "source": [
        "# Convert data to numerical format\n",
        "x = np.zeros((len(sentences), maxlen, len(vocab)), dtype=np.bool_)\n",
        "y = np.zeros((len(sentences), len(vocab)), dtype=np.bool_)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char2index[char]] = 1\n",
        "    y[i, char2index[next_chars[i]]] = 1\n",
        "\n",
        "#i: index of sentences\n",
        "#t: index of char in ith sentence\n",
        "#char2index -> index of that specific char in the vocab list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA9SPu_mEyFO",
        "outputId": "94dd71ca-4ca0-4b3f-cfa1-9c5ba503b279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x is : (371785, 40, 65)\n",
            "Shape of y is : (371785, 65)\n",
            "--------------------------------------------------\n",
            "x[0] is given as:\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "--------------------------------------------------\n",
            "y[0] is given as:\n",
            "[False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False  True False\n",
            " False False False False False]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of x is : {x.shape}\")\n",
        "print(f\"Shape of y is : {y.shape}\")\n",
        "print(f\"{'-'*50}\\nx[0] is given as:\\n{x[0]}\")\n",
        "print(f\"{'-'*50}\\ny[0] is given as:\\n{y[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBZj5JiIqmgy"
      },
      "source": [
        "#1. ***Text Generation via Stacked & Simple RNN***\n",
        "\n",
        "#Task:\n",
        " Using a dataset of your choice (e.g., text, time-series data), implement a basic RNN model. Train the model to perform a sequence task such as text generation, sentiment analysis, or time-series prediction.\n",
        "#Deliverable:\n",
        "   Perform this experimentation in a notebook and provide a detailed explanation or comments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npECjPN11opZ"
      },
      "source": [
        "##**Model Creation Function**:\n",
        "  - `create_stacked_rnn_model(vocab_size, embedding_dim, sequence_length, rnn_units)`:\n",
        "    - Defines a function to create a stacked Recurrent Neural Network (RNN) model for text generation.\n",
        "    - **Parameters**:\n",
        "      - `vocab_size`: Number of unique characters in the vocabulary.\n",
        "      - `embedding_dim`: Dimensionality of the embedding space (not used in this specific function but typically used for embedding layers).\n",
        "      - `sequence_length`: Length of input sequences for the RNN.\n",
        "      - `rnn_units`: Number of units in each RNN layer.\n",
        "\n",
        "- **Model Architecture**:\n",
        "  - **Input Layer**:\n",
        "    - `InputLayer(input_shape=(sequence_length, vocab_size))`:\n",
        "      - Specifies the input shape for the model. Here, `sequence_length` is the length of each input sequence, and `vocab_size` is the number of features per character (one-hot encoded).\n",
        "\n",
        "  - **RNN Layers**:\n",
        "    - `SimpleRNN(units=256, return_sequences=True)` (First Layer):\n",
        "      - The first RNN layer with 256 units and `return_sequences=True` outputs the full sequence of hidden states.\n",
        "    - `SimpleRNN(units=256, return_sequences=True)` (Second Layer):\n",
        "      - The second RNN layer, also with 256 units, maintains the sequence output.\n",
        "    - `SimpleRNN(units=256)` (Third Layer):\n",
        "      - The third RNN layer with 256 units processes the sequence and returns the final hidden state.\n",
        "\n",
        "  - **Output Layer**:\n",
        "    - `Dense(units=vocab_size, activation='softmax')`:\n",
        "      - The output layer with `vocab_size` units (equal to the number of unique characters) and a `softmax` activation function. This layer outputs probabilities for each character, allowing the model to predict the next character in the sequence.\n",
        "\n",
        "- **Model Summary**:\n",
        "  - `stacked_rnn.summary()`:\n",
        "    - Provides a summary of the model architecture, including layer types, output shapes, and parameter counts.\n",
        "  \n",
        "  - **Summary Output**:\n",
        "    - **Layers**:\n",
        "      - `simple_rnn_26 (SimpleRNN)`: Output shape `(None, 40, 256)` with 82,432 parameters.\n",
        "      - `simple_rnn_27 (SimpleRNN)`: Output shape `(None, 40, 256)` with 131,328 parameters.\n",
        "      - `simple_rnn_28 (SimpleRNN)`: Output shape `(None, 256)` with 131,328 parameters.\n",
        "      - `dense_9 (Dense)`: Output shape `(None, 65)` with 16,705 parameters.\n",
        "    - **Total Parameters**: 361,793 (1.38 MB), all of which are trainable.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4aqb7FvFi23",
        "outputId": "cc593ab4-f639-4256-ef56-2535512daf0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 40, 256)           82432     \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 40, 256)           131328    \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 256)               131328    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 65)                16705     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 361793 (1.38 MB)\n",
            "Trainable params: 361793 (1.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Build the model\n",
        "def create_stacked_rnn_model(vocab_size, embedding_dim,sequence_length, rnn_units):\n",
        "    model = Sequential([\n",
        "        # Input layer\n",
        "        InputLayer(input_shape=(sequence_length, vocab_size)),\n",
        "\n",
        "        # First SimpleRNN layer\n",
        "        SimpleRNN(units=256, return_sequences=True),\n",
        "\n",
        "        # Second SimpleRNN layer\n",
        "        SimpleRNN(units=256, return_sequences=True),\n",
        "\n",
        "        # Third SimpleRNN layer\n",
        "        SimpleRNN(units=256),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(units=vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "stacked_rnn=create_stacked_rnn_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=256,\n",
        "    sequence_length=maxlen,\n",
        "    rnn_units=256\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "stacked_rnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAURY6uY18L-"
      },
      "source": [
        "## Compiling the Stacked RNN Model\n",
        "\n",
        "- **Optimizer Setup**:\n",
        "  - `optimizer = RMSprop(learning_rate=0.01)`:\n",
        "    - Initializes the RMSprop optimizer with a learning rate of 0.01.\n",
        "    - **RMSprop**: An adaptive learning rate optimizer, which helps in handling the vanishing gradient problem and stabilizing training for RNNs by adjusting the learning rate based on recent gradient magnitudes.\n",
        "\n",
        "- **Model Compilation**:\n",
        "  - `stacked_rnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])`:\n",
        "    - **Loss Function**: `loss='categorical_crossentropy'`\n",
        "      - Used for multi-class classification problems. Measures the performance of the model by comparing the predicted probabilities with the actual class labels.\n",
        "      - Suitable for text generation tasks where the model predicts probabilities for each character in the vocabulary.\n",
        "\n",
        "    - **Optimizer**: `optimizer=optimizer`\n",
        "      - Specifies the RMSprop optimizer to be used for training the model.\n",
        "\n",
        "    - **Metrics**: `metrics=['accuracy']`\n",
        "      - Evaluates the model's performance based on accuracy, which measures the proportion of correct predictions to the total predictions.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P2sDlE7fGUJt"
      },
      "outputs": [],
      "source": [
        "# Step 3: Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "stacked_rnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-s10gNfmjGe"
      },
      "source": [
        "## **Setting Up Checkpoints for Model Training:**\n",
        "\n",
        "- **Creating Directory for Checkpoints**:\n",
        "  - `import os`: Imports the OS module for interacting with the operating system.\n",
        "  - `os.mkdir('./stacked_rnn_checkpoints')`:\n",
        "    - Creates a directory named `stacked_rnn_checkpoints` in the current working directory.\n",
        "    - This directory will be used to store model checkpoints during training.\n",
        "\n",
        "- **Checkpoint Directory and File Path**:\n",
        "  - `checkpoint_dir = './stacked_rnn_checkpoints'`:\n",
        "    - Specifies the path to the directory where model checkpoints will be saved.\n",
        "  - `checkpoint_path = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5.keras\")`:\n",
        "    - Defines the path and naming convention for the checkpoint files.\n",
        "    - `ckpt_{epoch}.weights.h5.keras` indicates that each checkpoint file will be named based on the epoch number, allowing for versioning of the model weights.\n",
        "\n",
        "- **ModelCheckpoint Callback**:\n",
        "  - `checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')`:\n",
        "    - **`checkpoint_path`**: Specifies where the model checkpoints will be saved.\n",
        "    - **`monitor='loss'`**: Monitors the training loss to determine when to save checkpoints.\n",
        "    - **`verbose=1`**: Provides detailed logs of checkpoint saving during training.\n",
        "    - **`save_best_only=True`**: Saves only the model weights that have the lowest loss, ensuring that only the best-performing model is kept.\n",
        "    - **`mode='min'`**: Configures the callback to save the model when the monitored loss is minimized.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RrqgaavWmqio"
      },
      "outputs": [],
      "source": [
        "#Making a dir to store checkpoints\n",
        "import os\n",
        "os.mkdir('./stacked_rnn_checkpoints')\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './stacked_rnn_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5.keras\")\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvjU98Se2p32"
      },
      "source": [
        "## **Training the Stacked RNN Model**\n",
        "\n",
        "- **Model Training**:\n",
        "  - `stackedRNN_track = stacked_rnn.fit(x, y, epochs=20, batch_size=128, validation_split=0.2, callbacks=[checkpoint])`:\n",
        "    - **`stacked_rnn.fit`**:\n",
        "      - Trains the `stacked_rnn` model using the provided data.\n",
        "    \n",
        "    - **Parameters**:\n",
        "      - `x`: Input data in one-hot encoded format (sequences of characters).\n",
        "      - `y`: Target data (next characters in one-hot encoded format).\n",
        "      - `epochs=20`: Number of training epochs. The model will be trained for 20 iterations over the entire dataset.\n",
        "      - `batch_size=128`: Number of samples per gradient update. Training will process data in batches of 128 sequences at a time.\n",
        "      - `validation_split=0.2`: Fraction of the training data to be used as validation data (20%). This helps in monitoring the model's performance on unseen data during training.\n",
        "      - `callbacks=[checkpoint]`: List of callback functions to apply during training. Here, `checkpoint` is used to save model weights based on the lowest loss.\n",
        "\n",
        "- **Tracking Training**:\n",
        "  - `stackedRNN_track`:\n",
        "    - Stores the training history returned by `fit()`, including metrics such as loss and accuracy for each epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnTjRtZioApe",
        "outputId": "44fcb054-aa48-472b-fce7-62fe9e4082d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 4.0263 - accuracy: 0.0752\n",
            "Epoch 1: loss improved from inf to 4.02630, saving model to ./stacked_rnn_checkpoints/ckpt_1.weights.h5.keras\n",
            "2324/2324 [==============================] - 286s 122ms/step - loss: 4.0263 - accuracy: 0.0752 - val_loss: 4.4702 - val_accuracy: 0.0618\n",
            "Epoch 2/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9905 - accuracy: 0.0796\n",
            "Epoch 2: loss improved from 4.02630 to 3.99054, saving model to ./stacked_rnn_checkpoints/ckpt_2.weights.h5.keras\n",
            "2324/2324 [==============================] - 281s 121ms/step - loss: 3.9905 - accuracy: 0.0796 - val_loss: 4.2513 - val_accuracy: 0.0701\n",
            "Epoch 3/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9890 - accuracy: 0.0809\n",
            "Epoch 3: loss improved from 3.99054 to 3.98904, saving model to ./stacked_rnn_checkpoints/ckpt_3.weights.h5.keras\n",
            "2324/2324 [==============================] - 281s 121ms/step - loss: 3.9890 - accuracy: 0.0809 - val_loss: 3.8837 - val_accuracy: 0.0419\n",
            "Epoch 4/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9940 - accuracy: 0.0797\n",
            "Epoch 4: loss did not improve from 3.98904\n",
            "2324/2324 [==============================] - 281s 121ms/step - loss: 3.9940 - accuracy: 0.0797 - val_loss: 4.1956 - val_accuracy: 0.0366\n",
            "Epoch 5/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9920 - accuracy: 0.0795\n",
            "Epoch 5: loss did not improve from 3.98904\n",
            "2324/2324 [==============================] - 281s 121ms/step - loss: 3.9920 - accuracy: 0.0795 - val_loss: 4.2742 - val_accuracy: 0.1191\n",
            "Epoch 6/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9945 - accuracy: 0.0794\n",
            "Epoch 6: loss did not improve from 3.98904\n",
            "2324/2324 [==============================] - 281s 121ms/step - loss: 3.9945 - accuracy: 0.0794 - val_loss: 4.1367 - val_accuracy: 0.0571\n",
            "Epoch 7/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9945 - accuracy: 0.0794\n",
            "Epoch 7: loss did not improve from 3.98904\n",
            "2324/2324 [==============================] - 281s 121ms/step - loss: 3.9945 - accuracy: 0.0794 - val_loss: 4.1623 - val_accuracy: 0.0431\n",
            "Epoch 8/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9926 - accuracy: 0.0795\n",
            "Epoch 8: loss did not improve from 3.98904\n",
            "2324/2324 [==============================] - 282s 121ms/step - loss: 3.9926 - accuracy: 0.0795 - val_loss: 3.9297 - val_accuracy: 0.1076\n",
            "Epoch 9/20\n",
            "2324/2324 [==============================] - ETA: 0s - loss: 3.9977 - accuracy: 0.0799\n",
            "Epoch 9: loss did not improve from 3.98904\n",
            "2324/2324 [==============================] - 281s 121ms/step - loss: 3.9977 - accuracy: 0.0799 - val_loss: 4.0979 - val_accuracy: 0.0403\n",
            "Epoch 10/20\n",
            " 998/2324 [===========>..................] - ETA: 2:24 - loss: 3.9984 - accuracy: 0.0800"
          ]
        }
      ],
      "source": [
        "#model training\n",
        "stackedRNN_track=stacked_rnn.fit(x,y,epochs=20,batch_size=128,validation_split=0.2,callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snqTOUP3tMwI"
      },
      "source": [
        "# 2. **Text Generation via Bi-directional RNNs**\n",
        "##Task:\n",
        " Modify your basic RNN model by stacking multiple RNN layers and also converting it into a bi-directional RNN. Analyze the performance improvement (if any) compared to the basic RNN model. (Note: Separate Implementation of Stacked RNN &  Bi-Directional RNN)\n",
        "## Deliverable:\n",
        "Perform this experimentation in a notebook and provide a detailed explanation or comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDe0Ue2V36Bz"
      },
      "source": [
        "## **Building a Bidirectional RNN Model**\n",
        "\n",
        "- **Building the Bidirectional RNN Model**:\n",
        "  - `from keras.layers import Bidirectional`:\n",
        "    - Imports the `Bidirectional` wrapper for RNN layers.\n",
        "\n",
        "  - **Model Creation Function**:\n",
        "    - `create_bidirectional_rnn_model(vocab_size, seq_length, embedding_dim, rnn_units)`:\n",
        "      - Defines a function to create a Bidirectional RNN model for text generation.\n",
        "      - **Parameters**:\n",
        "        - `vocab_size`: Number of unique characters in the vocabulary.\n",
        "        - `seq_length`: Length of input sequences for the RNN.\n",
        "        - `embedding_dim`: Dimensionality of the embedding space (not used in this specific function but typically used for embedding layers).\n",
        "        - `rnn_units`: Number of units in each RNN layer.\n",
        "\n",
        "    - **Model Architecture**:\n",
        "      - **Input Layer**:\n",
        "        - `InputLayer(input_shape=(seq_length, vocab_size))`:\n",
        "          - Specifies the input shape for the model. Each sequence has a length of `seq_length` with `vocab_size` features per character (one-hot encoded).\n",
        "\n",
        "      - **Bidirectional RNN Layers**:\n",
        "        - `Bidirectional(SimpleRNN(rnn_units, return_sequences=True))` (First Layer):\n",
        "          - A Bidirectional RNN layer with `rnn_units` units. The `Bidirectional` wrapper processes the sequence in both forward and backward directions, enhancing context understanding.\n",
        "          - `return_sequences=True` outputs the full sequence of hidden states.\n",
        "        - `Bidirectional(SimpleRNN(rnn_units))` (Second Layer):\n",
        "          - Another Bidirectional RNN layer with `rnn_units` units, providing the final hidden state.\n",
        "\n",
        "      - **Output Dense Layer**:\n",
        "        - `Dense(vocab_size, activation='softmax')`:\n",
        "          - Output layer with `vocab_size` units and a `softmax` activation function. Outputs probabilities for each character, used for predicting the next character in the sequence.\n",
        "\n",
        "  - **Model Creation**:\n",
        "    - `bidirectional_rnn = create_bidirectional_rnn_model(...)`:\n",
        "      - Creates the Bidirectional RNN model with specified parameters.\n",
        "\n",
        "  - **Model Summary**:\n",
        "      - `bidirectional_rnn.summary()`:\n",
        "      - Displays the model architecture, including layer types, output shapes, and parameter counts.\n",
        "\n",
        "- **Summary Output**:\n",
        "    - **Bidirectional Layers**:\n",
        "      - `bidirectional_6 (Bidirectional)`: Output shape `(None, 40, 1024)` with 591,872 parameters.\n",
        "      - `bidirectional_7 (Bidirectional)`: Output shape `(None, 1024)` with 1,573,888 parameters.\n",
        "    - **Dense Layer**:\n",
        "      - `dense_13 (Dense)`: Output shape `(None, 65)` with 66,625 parameters.\n",
        "    - **Total Parameters**: 2,232,385 (8.52 MB), all of which are trainable.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V1zXOJWtgwp"
      },
      "outputs": [],
      "source": [
        "#Building a BiDirectional RNN Model\n",
        "from keras.layers import  Bidirectional\n",
        "\n",
        "def create_bidirectional_rnn_model(vocab_size, seq_length, embedding_dim, rnn_units):\n",
        "\n",
        "    model = Sequential([\n",
        "        # Input layer\n",
        "        InputLayer(input_shape=(seq_length, vocab_size)),\n",
        "\n",
        "        #Bidirectional Layer\n",
        "        Bidirectional(SimpleRNN(rnn_units, return_sequences=True)),\n",
        "\n",
        "        #Bidirectional Layer\n",
        "        Bidirectional(SimpleRNN(rnn_units)),\n",
        "\n",
        "        #output Dense Layer\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create the bi-directional RNN model\n",
        "bidirectional_rnn = create_bidirectional_rnn_model(\n",
        "    vocab_size=len(vocab),\n",
        "    seq_length=maxlen,\n",
        "    embedding_dim=256,\n",
        "    rnn_units=512  # You can adjust the number of units\n",
        ")\n",
        "\n",
        "\n",
        "bidirectional_rnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMk-OLZL4fTa"
      },
      "source": [
        "## **Compiling the Model**:\n",
        "  - `bidirectional_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])`:\n",
        "    - **Optimizer**: `adam`\n",
        "      - Uses the Adam optimizer, which is an adaptive learning rate optimization algorithm.\n",
        "    - **Loss Function**: `categorical_crossentropy`\n",
        "      - Measures the performance of the model by comparing the predicted probabilities with the actual class labels.\n",
        "    - **Metrics**: `accuracy`\n",
        "      - Evaluates the model's performance based on accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NT1y8Slu1nP"
      },
      "outputs": [],
      "source": [
        "# Step 3: Compile the model\n",
        "\n",
        "bidirectional_rnn.compile(optimizer ='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnHR08hr40gq"
      },
      "source": [
        "## **Setting Up Checkpoints for Bidirectional RNN Training**\n",
        "\n",
        "- **Creating Directory for Checkpoints**:\n",
        "  - `import os`: Imports the OS module for interacting with the operating system.\n",
        "  - `os.mkdir('./Bidirectional_RNN_checkpoints')`:\n",
        "    - Creates a directory named `Bidirectional_RNN_checkpoints` in the current working directory.\n",
        "    - This directory will be used to store model checkpoints during training.\n",
        "\n",
        "- **Checkpoint Directory and File Path**:\n",
        "  - `checkpoint_dir = './Bidirectional_RNN_checkpoints'`:\n",
        "    - Specifies the path to the directory where model checkpoints will be saved. Note that this seems to be a different directory name than the one created. This discrepancy should be corrected for consistency.\n",
        "  - `checkpoint_path = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5.keras\")`:\n",
        "    - Defines the path and naming convention for the checkpoint files.\n",
        "    - `ckpt_{epoch}.weights.h5.keras` indicates that each checkpoint file will be named based on the epoch number, facilitating version control of the model weights.\n",
        "\n",
        "- **ModelCheckpoint Callback**:\n",
        "  - `checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')`:\n",
        "    - **`checkpoint_path`**: Specifies where the model checkpoints will be saved.\n",
        "    - **`monitor='loss'`**: Monitors the training loss to determine when to save checkpoints.\n",
        "    - **`verbose=1`**: Provides detailed logs of checkpoint saving during training.\n",
        "    - **`save_best_only=True`**: Saves only the model weights that have the lowest loss, ensuring that only the best-performing model is retained.\n",
        "    - **`mode='min'`**: Configures the callback to save the model when the monitored loss is minimized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5oG78w0vHRJ"
      },
      "outputs": [],
      "source": [
        "#Making a dir to store checkpoints\n",
        "import os\n",
        "os.mkdir('./Bidirectional_RNN_checkpoints')\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './Bidirectional_RNN_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5.keras\")\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNhDo_lc5RW4"
      },
      "source": [
        "## **Training the Bidirectional RNN Model**\n",
        "\n",
        "- **Model Training**:\n",
        "  - `bidir_track = bidirectional_rnn.fit(x, y, epochs=20, batch_size=128, validation_split=0.2, callbacks=[checkpoint])`:\n",
        "    - Trains the `bidirectional_rnn` model using the provided data.\n",
        "\n",
        "    - **Parameters**:\n",
        "      - `x`: Input data in one-hot encoded format (sequences of characters).\n",
        "      - `y`: Target data (next characters in one-hot encoded format).\n",
        "      - `epochs=20`: Number of training epochs. The model will be trained for 20 iterations over the entire dataset.\n",
        "      - `batch_size=128`: Number of samples per gradient update. Training will process data in batches of 128 sequences at a time.\n",
        "      - `validation_split=0.2`: Fraction of the training data to be used as validation data (20%). This helps in monitoring the model's performance on unseen data during training.\n",
        "      - `callbacks=[checkpoint]`: List of callback functions to apply during training. Here, `checkpoint` is used to save model weights based on the lowest loss.\n",
        "\n",
        "- **Tracking Training**:\n",
        "  - `bidir_track`:\n",
        "    - Stores the training history returned by `fit()`, including metrics such as loss and accuracy for each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxJPMeTEvPGW"
      },
      "outputs": [],
      "source": [
        "#model training\n",
        "bidir_track=bidirectional_rnn.fit(x,y,epochs=20,batch_size=128,validation_split=0.2,callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbv1-8WxveIO"
      },
      "source": [
        "# 3. **Exploring Hybrid Architectures**\n",
        "\n",
        "##Task:\n",
        "Implement a hybrid architecture by combining your RNN model with another model (e.g., CNN, Attention mechanism). Train this hybrid model on the same dataset and compare its performance with the previous models.\n",
        "##Deliverable:\n",
        "Submit the Python code in a notebook for the hybrid model along with a report discussing the results, challenges faced, and the benefits (or drawbacks) of using a hybrid approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3zJj2oy5z4N"
      },
      "source": [
        "## **Creating and Summarizing a Hybrid CNN-RNN Model**\n",
        "\n",
        "- **Model Creation**:\n",
        "  - `from keras.models import Model`:\n",
        "    - Imports the `Model` class for defining the model.\n",
        "\n",
        "  - **Defining the Hybrid Model Function**:\n",
        "    - `create_hybrid_model(vocab_size, rnn_units, maxlen)`:\n",
        "      - Function to create a hybrid model combining Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) components.\n",
        "      - **Parameters**:\n",
        "        - `vocab_size`: Number of unique characters in the vocabulary.\n",
        "        - `rnn_units`: Number of units in the RNN layer.\n",
        "        - `maxlen`: Length of input sequences.\n",
        "\n",
        "    - **Model Architecture**:\n",
        "      - **Input Layer**:\n",
        "        - `inputs = Input(shape=(maxlen, vocab_size))`:\n",
        "          - Specifies the input shape: sequences of length `maxlen` with `vocab_size` features (one-hot encoded).\n",
        "\n",
        "      - **CNN Part**:\n",
        "        - `conv1 = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs)`:\n",
        "          - 1D Convolutional layer with 64 filters, a kernel size of 5, and ReLU activation. Extracts features from the input sequences.\n",
        "        - `pool1 = MaxPooling1D(pool_size=2)(conv1)`:\n",
        "          - MaxPooling layer reduces the dimensionality of the feature maps, pooling over a window of size 2.\n",
        "\n",
        "        - `global_pool = GlobalMaxPooling1D()(pool1)`:\n",
        "          - GlobalMaxPooling layer reduces the sequence length to a single value per feature map by taking the maximum value. This creates a fixed-size representation from variable-length sequences.\n",
        "\n",
        "      - **RNN Part**:\n",
        "        - `rnn = SimpleRNN(rnn_units, return_sequences=False)(inputs)`:\n",
        "          - SimpleRNN layer with `rnn_units` units. `return_sequences=False` outputs the final hidden state for each sequence.\n",
        "\n",
        "      - **Combining CNN and RNN Outputs**:\n",
        "        - `concat = Concatenate()([global_pool, rnn])`:\n",
        "          - Concatenates the outputs from the CNN and RNN parts into a single vector.\n",
        "\n",
        "      - **Dense Layers**:\n",
        "        - `dense1 = Dense(128, activation='relu')(concat)`:\n",
        "          - Dense layer with 128 units and ReLU activation, providing a final learned representation from the combined features.\n",
        "        - `outputs = Dense(vocab_size, activation='softmax')(dense1)`:\n",
        "          - Output Dense layer with `vocab_size` units and softmax activation for predicting the next character probabilities.\n",
        "\n",
        "    - **Creating the Model**:\n",
        "      - `model = Model(inputs=inputs, outputs=outputs)`:\n",
        "        - Defines the Keras model using the specified inputs and outputs.\n",
        "\n",
        "  - **Model Creation**:\n",
        "    - `hybrid_model = create_hybrid_model(...)`:\n",
        "      - Instantiates the hybrid model with given parameters.\n",
        "\n",
        "  - **Model Summary**:\n",
        "    - `hybrid_model.summary()`:\n",
        "      - Displays the model architecture including layer types, output shapes, and parameter counts.\n",
        "\n",
        "    - **Summary Output**:\n",
        "      - **Conv1D Layer**:\n",
        "        - `conv1d_21 (Conv1D)`: Output shape `(None, 36, 64)` with 20,864 parameters.\n",
        "      - **MaxPooling1D Layer**:\n",
        "        - `max_pooling1d_10`: Output shape `(None, 18, 64)`.\n",
        "      - **GlobalMaxPooling1D Layer**:\n",
        "        - `global_max_pooling1d_4`: Output shape `(None, 64)`.\n",
        "      - **SimpleRNN Layer**:\n",
        "        - `simple_rnn_49 (SimpleRNN)`: Output shape `(None, 512)` with 295,936 parameters.\n",
        "      - **Concatenate Layer**:\n",
        "        - `concatenate_9`: Combines CNN and RNN outputs with output shape `(None, 576)`.\n",
        "      - **Dense Layers**:\n",
        "        - `dense_26 (Dense)`: Output shape `(None, 128)` with 73,856 parameters.\n",
        "        - `dense_27 (Dense)`: Output shape `(None, 65)` with 8,385 parameters.\n",
        "      - **Total Parameters**: 399,041 (1.52 MB), all trainable.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDN8EoZ1vwHF"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Concatenate, GlobalMaxPooling1D\n",
        "\n",
        "def create_hybrid_model(vocab_size, rnn_units, maxlen):\n",
        "\n",
        "    # Input layer (no Embedding layer since input is one-hot encoded)\n",
        "    inputs = Input(shape=(maxlen, vocab_size))  # Adjust input shape to (40, 65)\n",
        "\n",
        "    # CNN part\n",
        "    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "\n",
        "    # Reduce sequence length using GlobalMaxPooling1D\n",
        "    global_pool = GlobalMaxPooling1D()(pool1)\n",
        "\n",
        "    # RNN part\n",
        "    rnn = SimpleRNN(rnn_units, return_sequences=False)(inputs)  # Use original one-hot input for RNN\n",
        "\n",
        "    # Combine CNN and RNN\n",
        "    concat = Concatenate()([global_pool, rnn])\n",
        "\n",
        "    # Dense layers\n",
        "    dense1 = Dense(128, activation='relu')(concat)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(dense1)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and train the hybrid model\n",
        "hybrid_model = create_hybrid_model(\n",
        "    maxlen=maxlen,\n",
        "    rnn_units = 512,\n",
        "\n",
        "    vocab_size=len(vocab)\n",
        ")\n",
        "hybrid_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPWRoQ336wAI"
      },
      "source": [
        "## **Compiling the Model**:\n",
        "  - `bidirectional_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])`:\n",
        "    - **Optimizer**: `adam`\n",
        "      - Uses the Adam optimizer, which is an adaptive learning rate optimization algorithm.\n",
        "    - **Loss Function**: `categorical_crossentropy`\n",
        "      - Measures the performance of the model by comparing the predicted probabilities with the actual class labels.\n",
        "    - **Metrics**: `accuracy`\n",
        "      - Evaluates the model's performance based on accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHE_yARZwz2L"
      },
      "outputs": [],
      "source": [
        "#compile the model\n",
        "hybrid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSu6hfBE6Sxw"
      },
      "source": [
        "## **Setting Up Checkpoints for Hybrid CNN-RNN Model**\n",
        "\n",
        "- **Creating Directory for Checkpoints**:\n",
        "  - `import os`: Imports the OS module for filesystem operations.\n",
        "  - `os.mkdir('./HybridModel_checkpoints')`:\n",
        "    - Creates a directory named `HybridModel_checkpoints` in the current working directory.\n",
        "    - This directory will store the checkpoints during the training of the hybrid model.\n",
        "\n",
        "- **Checkpoint Directory and File Path**:\n",
        "  - `checkpoint_dir = './HybridModel_checkpoints'`:\n",
        "    - Specifies the path to the directory where model checkpoints will be saved.\n",
        "  - `checkpoint_path = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5.keras\")`:\n",
        "    - Defines the file path and naming convention for the checkpoint files.\n",
        "    - `ckpt_{epoch}.weights.h5.keras` indicates that each checkpoint file will be named according to the epoch number, allowing for easy tracking of different model versions.\n",
        "\n",
        "- **ModelCheckpoint Callback**:\n",
        "  - `checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')`:\n",
        "    - **`checkpoint_path`**: Path where the model checkpoints will be saved.\n",
        "    - **`monitor='loss'`**: Monitors the training loss to decide when to save the model.\n",
        "    - **`verbose=1`**: Provides detailed output on checkpoint saving during training.\n",
        "    - **`save_best_only=True`**: Saves only the model weights with the lowest loss, ensuring that the best-performing model is kept.\n",
        "    - **`mode='min'`**: Configures the callback to save the model whenever the monitored loss decreases, thus ensuring the lowest loss model is saved.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwsaie0lw9ru"
      },
      "outputs": [],
      "source": [
        "#Making a dir to store checkpoints\n",
        "import os\n",
        "os.mkdir('./HybridModel_checkpoints')\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './HybridModel_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5.keras\")\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukK2w6r46lJC"
      },
      "source": [
        "## **Training the Bidirectional RNN Model**\n",
        "\n",
        "- **Model Training**:\n",
        "  - `bidir_track = bidirectional_rnn.fit(x, y, epochs=20, batch_size=128, validation_split=0.2, callbacks=[checkpoint])`:\n",
        "    - Trains the `bidirectional_rnn` model using the provided data.\n",
        "\n",
        "    - **Parameters**:\n",
        "      - `x`: Input data in one-hot encoded format (sequences of characters).\n",
        "      - `y`: Target data (next characters in one-hot encoded format).\n",
        "      - `epochs=20`: Number of training epochs. The model will be trained for 20 iterations over the entire dataset.\n",
        "      - `batch_size=128`: Number of samples per gradient update. Training will process data in batches of 128 sequences at a time.\n",
        "      - `validation_split=0.2`: Fraction of the training data to be used as validation data (20%). This helps in monitoring the model's performance on unseen data during training.\n",
        "      - `callbacks=[checkpoint]`: List of callback functions to apply during training. Here, `checkpoint` is used to save model weights based on the lowest loss.\n",
        "\n",
        "- **Tracking Training**:\n",
        "  - `bidir_track`:\n",
        "    - Stores the training history returned by `fit()`, including metrics such as loss and accuracy for each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "msiSogfpxIYE"
      },
      "outputs": [],
      "source": [
        "#model training\n",
        "track=hybrid_model.fit(x,y,epochs=20,batch_size=128,validation_split=0.2,callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkoNfFVTI_0q"
      },
      "source": [
        "#Step 4: Generate Text\n",
        "- Define a function to generate text based on a given seed text\n",
        "- Use the trained model to predict the next character at each step\n",
        "- Append the predicted character to the generated text and update the seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N-Fg1spNWSm"
      },
      "source": [
        "## **Sample Function**\n",
        "### Function Definition\n",
        "\n",
        "This line defines a function named `sample`, which takes two arguments:\n",
        "- `preds` (the predicted probabilities of the next characters)\n",
        "- `temperature` (a parameter that controls the randomness of predictions).\n",
        "\n",
        "### 1. Convert to NumPy Array\n",
        "\n",
        "This line converts the `preds` input into a NumPy array and ensures that its data type is `float64`. This is important for numerical stability and for performing mathematical operations.\n",
        "\n",
        "### 2. Logarithm and Temperature Scaling\n",
        "\n",
        "The logarithm of the predicted probabilities is computed, and then it is divided by the temperature. Lower temperatures (e.g., < 1.0) make the model more confident, leading to less random predictions, while higher temperatures (e.g., > 1.0) increase randomness.\n",
        "\n",
        "### 3. Exponential of Logarithm\n",
        "\n",
        "This line calculates the exponential of the scaled log probabilities. This step transforms the log probabilities back to the original scale, but now adjusted by the temperature.\n",
        "\n",
        "### 4. Normalization\n",
        "\n",
        "The probabilities are normalized by dividing each value by the sum of all exponentials. This ensures that the resulting probabilities sum to 1, making it a valid probability distribution.\n",
        "\n",
        "### 5. Sampling from the Distribution\n",
        "\n",
        "This line uses a multinomial distribution to sample one outcome based on the probabilities in `preds`. The `np.random.multinomial(1, preds, 1)` function returns an array where the index corresponding to the sampled character will have a value of 1, and all others will be 0.\n",
        "\n",
        "### 6. Return the Index of the Sampled Character\n",
        "\n",
        "The function returns the index of the character that was sampled by finding the index of the maximum value in the `probas` array. This index corresponds to the next character to be generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6Cqe2gUPEnz"
      },
      "outputs": [],
      "source": [
        "# Step 5: Text generation function\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmkckE0mPV2L"
      },
      "source": [
        "## The `generate_text` Function\n",
        "\n",
        "### Function Definition\n",
        "\n",
        "This line defines the `generate_text` function, which takes three parameters:\n",
        "- **`seed_text`**: The initial text to start generation.\n",
        "- **`num_generated`**: The number of characters to generate.\n",
        "- **`temperature`**: Controls randomness.\n",
        "\n",
        "### Initialize Generated Text\n",
        "\n",
        "The variable `generated_text` is initialized with the `seed_text`. This will be modified as new characters are generated.\n",
        "\n",
        "### Loop for Character Generation\n",
        "\n",
        "This line starts a loop that will run `num_generated` times, generating one character in each iteration.\n",
        "\n",
        "### Prepare Input for the Model\n",
        "\n",
        "A NumPy array named `encoded_text` is created with shape `(1, seq_length, vocab_size)`, initialized to zeros. This array will hold the one-hot encoded representation of the last `seq_length` characters from `generated_text`.\n",
        "\n",
        "### Iterate Over the Last `seq_length` Characters\n",
        "\n",
        "This loop iterates over the last `seq_length` characters of `generated_text`, allowing the model to predict the next character based on this context.\n",
        "\n",
        "### One-Hot Encoding\n",
        "\n",
        "For each character in the last `seq_length` characters, this line sets the corresponding index in `encoded_text` to 1, effectively creating a one-hot encoded representation of the input sequence.\n",
        "\n",
        "### Predict the Next Character\n",
        "\n",
        "The model is called to predict the next character based on the `encoded_text`. The `verbose=0` argument suppresses output during prediction. The result is a probability distribution over the vocabulary for the next character.\n",
        "\n",
        "### Sample the Next Character Index\n",
        "\n",
        "The `sample` function is called with the predicted probabilities and the specified `temperature`. This returns the index of the next character to be generated.\n",
        "\n",
        "### Get the Next Character\n",
        "\n",
        "The character corresponding to the sampled index is retrieved from the `index_to_char` mapping.\n",
        "\n",
        "### Append the Next Character\n",
        "\n",
        "The newly generated character is appended to `generated_text`, extending the text.\n",
        "\n",
        "### Return the Generated Text\n",
        "\n",
        "After the loop completes, the function returns the full generated text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbIL52_GQGt0"
      },
      "outputs": [],
      "source": [
        "def generate_text(model_name,seed_text, num_generated, temperature=0.5):\n",
        "    generated_text = seed_text\n",
        "    model=model_name\n",
        "    for i in range(num_generated):\n",
        "        encoded_text = np.zeros((1, maxlen, len(vocab)))\n",
        "        for t, char in enumerate(generated_text[-maxlen:]):\n",
        "            encoded_text[0, t, char2index[char]] = 1.\n",
        "\n",
        "        preds = model.predict(encoded_text, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = index2char[next_index]\n",
        "\n",
        "        generated_text += next_char\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1QYlYHGQ744"
      },
      "outputs": [],
      "source": [
        "#For simple RNN\n",
        "generated_text = generate_text(model_name=model,\n",
        "                               seed_text = \"You are all resolved rather to die than to famish?\",\n",
        "                               num_generated=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ0Wxv6jyd3z"
      },
      "outputs": [],
      "source": [
        "#For Bidirectional RNN\n",
        "generated_text = generate_text(model_name=bidirectional_rnn,\n",
        "                               seed_text = \"You are all resolved rather to die than to famish?\",\n",
        "                               num_generated=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTQfF18IyoJF"
      },
      "outputs": [],
      "source": [
        "#For Hybrid Model\n",
        "generated_text = generate_text(model_name=hybrid,\n",
        "                               seed_text = \"You are all resolved rather to die than to famish?\",\n",
        "                               num_generated=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Text Generation using Recurrent Neural Networks: An Analytical Report***\n",
        "\n",
        "---\n",
        "\\\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "This report analyzes the implementation of various Recurrent Neural Network (RNN) architectures for text generation. The project utilizes a dataset of Shakespeare's works to train and evaluate different models, including a simple RNN, a bidirectional RNN, and a hybrid CNN-RNN model.\n",
        "\n",
        "### Objectives:\n",
        "1. Implement and compare different RNN architectures for text generation.\n",
        "2. Analyze the performance of each model in terms of text generation quality.\n",
        "3. Explore the impact of different model architectures on training efficiency and output quality.\n",
        "\n",
        "### Dataset:\n",
        "The dataset consists of Shakespeare's texts, containing 1,115,394 characters with 65 unique characters in the vocabulary.\n",
        "\n",
        "## 2. Data Analysis and Preprocessing\n",
        "\n",
        "### 2.1 Text Analysis\n",
        "- Total characters: 1,115,394\n",
        "- Unique characters: 65\n",
        "- Vocabulary includes lowercase and uppercase letters, punctuation, and special characters\n",
        "\n",
        "### 2.2 Data Preprocessing\n",
        "- The text was converted to a sequence of indices using character-to-index mapping.\n",
        "- Input sequences of length 40 were created with a step size of 3, resulting in 371,785 sequences.\n",
        "- One-hot encoding was applied to both input sequences and target characters.\n",
        "\n",
        "### 2.3 Input Shape\n",
        "- X shape: (371,785, 40, 65)\n",
        "- Y shape: (371,785, 65)\n",
        "\n",
        "## 3. Model Building\n",
        "\n",
        "Three different model architectures were implemented:\n",
        "\n",
        "### 3.1 Simple Stacked RNN\n",
        "- Architecture:\n",
        "  - Input layer\n",
        "  - 3 SimpleRNN layers (256 units each)\n",
        "  - Dense output layer (65 units, softmax activation)\n",
        "- Total parameters: 361,793\n",
        "\n",
        "### 3.2 Bidirectional RNN\n",
        "- Architecture:\n",
        "  - Input layer\n",
        "  - 2 Bidirectional SimpleRNN layers (512 units each)\n",
        "  - Dense output layer (65 units, softmax activation)\n",
        "- Total parameters: 2,232,385\n",
        "\n",
        "### 3.3 Hybrid CNN-RNN Model\n",
        "- Architecture:\n",
        "  - Input layer\n",
        "  - CNN part: Conv1D (64 filters) -> MaxPooling1D -> GlobalMaxPooling1D\n",
        "  - RNN part: SimpleRNN (512 units)\n",
        "  - Concatenation of CNN and RNN outputs\n",
        "  - Dense layer (128 units)\n",
        "  - Output Dense layer (65 units, softmax activation)\n",
        "- Total parameters: 399,041\n",
        "\n",
        "## 4. Model Training\n",
        "\n",
        "All models were trained with the following configuration:\n",
        "- Optimizer: Adam\n",
        "- Loss function: Categorical cross-entropy\n",
        "- Metrics: Accuracy\n",
        "- Batch size: 128\n",
        "- Epochs: 20\n",
        "- Validation split: 0.2\n",
        "\n",
        "### 4.1 Training Results\n",
        "\n",
        "Only the Hybrid CNN-RNN model's training results were provided:\n",
        "\n",
        "- Initial loss: 2.7264\n",
        "- Final loss (after 13 epochs): 1.5706\n",
        "- Initial accuracy: 0.2735\n",
        "- Final accuracy (after 13 epochs): 0.5242\n",
        "- Best validation accuracy: 0.4649 (Epoch 9)\n",
        "- Best validation loss: 1.8889 (Epoch 9)\n",
        "\n",
        "The model showed consistent improvement in both training loss and accuracy over the epochs, with some indications of potential overfitting as the validation metrics did not improve as rapidly.\n",
        "\n",
        "## 5. Text Generation\n",
        "\n",
        "A temperature-based sampling method was implemented for text generation:\n",
        "- Lower temperatures (< 1.0) result in more deterministic outputs\n",
        "- Higher temperatures (> 1.0) increase randomness in generated text\n",
        "\n",
        "Text generation was performed using all three models with the seed text: \"You are all resolved rather to die than to famish?\"\n",
        "\n",
        "### 5.1 Simple RNN Output\n",
        "\"You are all resolved rather to die than to famish?\n",
        "LADY CAPULET:\n",
        "What like the tanes with a war, and while they are the deeds\n",
        "That they hath the like\"\n",
        "\n",
        "### 5.2 Bidirectional RNN and Hybrid Model\n",
        "Outputs for these models were not provided in the document.\n",
        "\n",
        "## 6. Analysis and Insights\n",
        "\n",
        "1. Model Complexity:\n",
        "   - The Bidirectional RNN has the highest number of parameters (2,232,385), followed by the Hybrid model (399,041), and then the Simple RNN (361,793).\n",
        "   - The increased complexity of the Bidirectional RNN might lead to better context understanding but could also risk overfitting on smaller datasets.\n",
        "\n",
        "2. Training Performance:\n",
        "   - The Hybrid model showed steady improvement in both loss and accuracy over the training epochs.\n",
        "   - There are signs of potential overfitting, as the validation metrics did not improve as rapidly as the training metrics.\n",
        "\n",
        "3. Text Generation Quality:\n",
        "   - The sample output from the Simple RNN shows that the model has learned some structure of the language, including character names and sentence formation.\n",
        "   - However, the generated text lacks coherence and proper context, indicating room for improvement.\n",
        "\n",
        "4. Architecture Comparison:\n",
        "   - The Hybrid model's combination of CNN and RNN layers might capture both local and long-range dependencies in the text.\n",
        "   - The Bidirectional RNN's ability to process sequences in both directions could potentially lead to better context understanding compared to the Simple RNN.\n",
        "\n",
        "## 7. Conclusion and Future Work\n",
        "\n",
        "The implementation and comparison of different RNN architectures for text generation provide valuable insights into the strengths and limitations of each approach. While the models show promising results in learning the structure of Shakespearean text, there is significant room for improvement in generating coherent and contextually appropriate passages.\n",
        "\n",
        "Future work could include:\n",
        "1. Experimenting with longer training periods and larger datasets.\n",
        "2. Implementing more advanced architectures like LSTMs or Transformers.\n",
        "3. Fine-tuning hyperparameters, especially the temperature for text generation.\n",
        "4. Conducting a more rigorous evaluation of generated text quality using metrics like perplexity or human evaluation.\n",
        "5. Exploring transfer learning approaches by pre-training on larger corpora before fine-tuning on Shakespeare's works.\n",
        "\n",
        "By addressing these areas, it may be possible to achieve more coherent and stylistically accurate text generation that captures the essence of Shakespearean language."
      ],
      "metadata": {
        "id": "w-HT6t9QkIvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART No. 01 : Understanding of RNN**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##**Question 1:** What are Recurrent Neural Networks, and how do they differ from traditional feedforward neural networks?\n",
        "\n",
        "### **Understanding RNN**\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks designed for processing sequential data. Unlike traditional feedforward neural networks, where information moves in one direction (from input to output), RNNs have connections that loop back on themselves. This allows RNNs to maintain a hidden state, which acts as a form of memory, enabling them to retain information from previous inputs in the sequence.\n",
        "\n",
        "In RNNs, the output at each time step is influenced not only by the current input but also by the previous hidden state, allowing the network to learn temporal dependencies. This is particularly useful for tasks such as language modeling, where the context of previous words is crucial for predicting the next word in a sequence.\n",
        "\n",
        "---\n",
        "\\\n",
        "\n",
        "##**Question 2:** Discuss the advantages and potential drawbacks of stacking RNN layers. What are Bi-directional RNNs, and how do they enhance the performance of sequence models?\n",
        "\n",
        "### **Stacking RNN Layers and Bi-directional Architecture**\n",
        "\n",
        "Stacking RNN layers can enhance the model's ability to learn complex patterns in the data. By adding more layers, the model can capture higher-level abstractions and temporal dependencies. However, stacking RNN layers also introduces challenges, such as increased computational complexity and a higher risk of overfitting, especially if the training data is limited.\n",
        "\n",
        "Bi-directional RNNs are a type of RNN that processes the input sequence in both forward and backward directions. This means that the model has access to both past and future context when making predictions. By doing so, bi-directional RNNs can improve performance on tasks where understanding the entire context of the sequence is important, such as in natural language processing tasks.\n",
        "\n",
        "---\n",
        "\\\n",
        "\n",
        "##**Question 3:** What is a hybrid architecture in the context of sequence modeling? Provide examples of how combining RNNs with other deep learning models can enhance performance.\n",
        "\n",
        "### **Hybrid Architecture**\n",
        "\n",
        "A hybrid architecture in sequence modeling refers to the combination of different types of neural networks to leverage their strengths. For example, combining RNNs with Convolutional Neural Networks (CNNs) can enhance performance by allowing the model to capture local patterns in the input data while also maintaining the ability to learn from sequences.\n",
        "\n",
        "One common approach is to use CNN layers to extract features from the input data (such as text or time series) and then feed those features into RNN layers for sequential processing. This combination can improve the model's ability to learn both spatial and temporal dependencies, leading to better performance on tasks such as sentiment analysis or video classification.\n",
        "\n",
        "---\n",
        "\\\n",
        "\n",
        "##**Question 4:** List down types of RNN model and explain their structures and differences with RNN.\n",
        "\n",
        "\n",
        "### **Types of RNN**\n",
        "1. **Simple RNN**: The basic form of RNN, where the output from the previous time step is fed back into the network. It has a straightforward architecture but struggles with long-term dependencies due to issues like vanishing gradients.\n",
        "\n",
        "2. **LSTM (Long Short-Term Memory)**: An advanced type of RNN designed to overcome the limitations of simple RNNs. LSTMs have a more complex structure with gates (input, forget, and output gates) that control the flow of information, allowing them to remember information for longer periods.\n",
        "\n",
        "3. **GRU (Gated Recurrent Unit)**: A simplified version of LSTM that combines the forget and input gates into a single update gate. GRUs are computationally more efficient than LSTMs while still capturing long-term dependencies.\n",
        "\n",
        "4. **Bidirectional RNN**: An RNN that processes the input sequence in both directions (forward and backward). This allows the model to have access to future context in addition to past context, enhancing performance on tasks where the entire sequence context is important.\n",
        "\n",
        "Each of these RNN types has its own structure and is suited for different tasks based on the specific requirements of the data and the problem being solved.\n",
        "\n"
      ],
      "metadata": {
        "id": "z0cqL_Sfhqfe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}